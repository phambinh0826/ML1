{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dceaa4e5-161e-486b-940a-52d166f5969c",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d12b56-34c8-4d9c-a7ee-c9e8c39d97d8",
   "metadata": {},
   "source": [
    "# **Mô Hình AI Đầu Tiên Của Bạn Sử Dụng Python và Scikit-learn**\n",
    "\n",
    "Thời gian ước tính cần: **30** phút\n",
    "\n",
    "Trong bài hướng dẫn về khoa học dữ liệu và học máy này, bạn sẽ được thực hành tạo và chạy một mô hình phân loại từ đầu đến cuối. Bài hướng dẫn bao gồm các bước sau:\n",
    "\n",
    "- Khám phá dữ liệu\n",
    "- Tiền xử lý dữ liệu\n",
    "- Chia dữ liệu để huấn luyện và kiểm tra\n",
    "- Chuẩn bị mô hình phân loại\n",
    "- Ghép nối tất cả các bước lại với nhau sử dụng pipeline\n",
    "- Huấn luyện mô hình\n",
    "- Chạy dự đoán trên mô hình\n",
    "- Đánh giá và trực quan hóa hiệu suất của mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5876f3-a082-4ab3-ac2e-b03fd6980221",
   "metadata": {},
   "source": [
    "# __Mục Lục__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Mục tiêu\">Mục tiêu</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Cài đặt\">Cài đặt</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Cài-Đặt-Thư-Viện-Cần-Thiết\">Cài Đặt Thư Viện Cần Thiết</a></li>\n",
    "            <li><a href=\"#Nhập-Thư-Viện-Cần-Thiết\">Nhập Thư Viện Cần Thiết</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Khám-phá-dữ-liệu\">Khám phá dữ liệu</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Về-bộ-dữ-liệu\">Về bộ dữ liệu</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Tiền-xử-lý-dữ-liệu\">Tiền xử lý dữ liệu</a>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Mô-hình-hóa\">Mô hình hóa</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Chia-dữ-liệu-cho-huấn-luyện-và-kiểm-tra\">Chia dữ liệu cho huấn luyện và kiểm tra</a></li>\n",
    "            <li><a href=\"#Chuẩn-bị-mô-hình-phân-loại\">Chuẩn bị mô hình phân loại</a></li>\n",
    "            <li><a href=\"#Ghép-nối-các-bước-sử-dụng-pipeline\">Ghép nối các bước sử dụng pipeline</a></li>\n",
    "            <li><a href=\"#Huấn-luyện-mô-hình\">Huấn luyện mô hình</a></li>\n",
    "            <li><a href=\"#Chạy-dự-đoán-trên-mô-hình\">Chạy dự đoán trên mô hình</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Đánh-giá-và-trực-quan-hóa-hiệu-suất-mô-hình\">Đánh giá và trực quan hóa hiệu suất mô hình</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Phương-pháp-hỗ-trợ-cho-việc-tạo-biểu-đồ\">Phương pháp hỗ trợ cho việc tạo biểu đồ</a></li>\n",
    "            <li><a href=\"#Trực-quan-hóa-2D-và-3D-của-kết-quả\">Trực quan hóa 2D và 3D của kết quả</a></li>\n",
    "            <li><a href=\"#Ma-trận-bối-rối\">Ma trận bối rối</a></li>\n",
    "            <li><a href=\"#Nghiên-cứu-so-sánh\">Nghiên cứu so sánh</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Tóm-tắt\">Tóm tắt</a>\n",
    "    </li>\n",
    "    <li"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7a8774-f7e9-4660-a30b-09116a277206",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db517e2-ee29-4bbd-9e58-aab6148513d1",
   "metadata": {},
   "source": [
    "# Mục Tiêu\n",
    "\n",
    "Sau khi hoàn thành bài thực hành này, bạn sẽ có thể:\n",
    "\n",
    " - Hiểu và áp dụng các nguyên lý cơ bản của học máy sử dụng Python và scikit-learn.\n",
    " - Phát triển khả năng sử dụng các thuật toán phân loại, đặc biệt là phương pháp phân loại Random Forest.\n",
    " - Tích lũy kinh nghiệm thực tế trong việc xử lý và phân tích dữ liệu thực từ các nền tảng giao dịch trực tuyến để dự đoán sự rời bỏ khách hàng.\n",
    " - Trực quan hóa dữ liệu phức tạp dưới cả định dạng 2D và 3D bằng cách sử dụng các thư viện matplotlib và scikitplot, nâng cao khả năng giải thích và trình bày những hiểu biết dựa trên dữ liệu.\n",
    " - Triển khai các mô hình học máy để hỗ trợ ra quyết định kinh doanh trong môi trường cạnh tranh cao, đặc biệt là trong chiến lược duy trì và cải thiện sự hài lòng của khách hàng."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20b7e74-cd4c-4913-bcbe-83eed88a5f3a",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966641f5-e1a8-4ef6-a8d9-34073c48c491",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1abae76-031e-4e75-8634-9df5fc17c83a",
   "metadata": {},
   "source": [
    "Trong bài thực hành này, chúng ta sẽ sử dụng các thư viện sau:\n",
    "\n",
    "*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) để quản lý dữ liệu.\n",
    "*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) để thực hiện các phép toán toán học.\n",
    "*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) cho các chức năng học máy và các chức năng liên quan đến pipeline trong học máy.\n",
    "*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) để trực quan hóa dữ liệu.\n",
    "*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) để cung cấp các công cụ vẽ bổ sung.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16af331a-0a99-46be-b3db-1110c4aca7cf",
   "metadata": {},
   "source": [
    "### Cài Đặt Thư Viện Cần Thiết\n",
    "\n",
    "Các thư viện cần thiết dưới đây đã được cài sẵn trong môi trường Skills Network Labs. Tuy nhiên, nếu bạn chạy các lệnh trong notebook này trên một môi trường Jupyter khác (ví dụ: Watson Studio hoặc Anaconda), bạn sẽ cần phải cài đặt các thư viện này bằng cách bỏ dấu `#` trước `!pip` trong ô mã dưới đây."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4264c70-31d8-44ad-a033-359ddcd5d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n",
    "# !pip install -qy pandas==1.3.4 matplotlib==3.5.0 scikit-learn==0.20.1\n",
    "# - Update a specific package\n",
    "# !pip install pmdarima -U\n",
    "# - Update a package to specific version\n",
    "# !pip install --upgrade pmdarima==2.0.2\n",
    "# Note: If your environment doesn't support \"!pip install\", use \"!mamba install\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a359afa-ae8b-40f9-8c3d-050425424ad0",
   "metadata": {},
   "source": [
    "Các thư viện cần thiết dưới đây __không__ được cài sẵn trong môi trường Skills Network Labs. __Bạn sẽ cần chạy ô mã dưới đây__ để cài đặt chúng:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ed1789a-862f-4449-8701-6c25de7b5d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"tqdm == 4.60.0\"\n",
    "#!pip install seaborn\n",
    "#!pip install \"skillsnetwork == 0.20.6\"\n",
    "#!pip install \"scikit-plot == 0.3.7\"\n",
    "#!pip install numpy\n",
    "#!pip install pandas\n",
    "#!pip install matplotlib\n",
    "#!pip install scikit-learn\n",
    "#!pip install scipy==1.9.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf63cce-5e44-4ab9-963b-e9ecc59930ad",
   "metadata": {},
   "source": [
    "### Nhập Thư Viện Cần Thiết\n",
    "\n",
    "_Chúng tôi khuyến nghị bạn nhập tất cả các thư viện cần thiết ở một nơi (ở đây):_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf483ec1-3299-4c59-8f46-cc88be191e8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'interp' from 'scipy' (c:\\Users\\minh PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpatches\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmpatches\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscikitplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mskplt\u001b[39;00m\n\u001b[32m     19\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mmatplotlib\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minline\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, OneHotEncoder, LabelEncoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\minh PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikitplot\\__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m absolute_import, division, print_function, unicode_literals\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m metrics, cluster, decomposition, estimators\n\u001b[32m      3\u001b[39m __version__ = \u001b[33m'\u001b[39m\u001b[33m0.3.7\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscikitplot\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassifiers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classifier_factory\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\minh PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikitplot\\metrics.py:27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcalibration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m calibration_curve\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interp\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscikitplot\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhelpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m binary_ks_curve, validate_labels\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscikitplot\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhelpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cumulative_gain_curve\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'interp' from 'scipy' (c:\\Users\\minh PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# You can use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warn()\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import skillsnetwork\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import accumulate\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import scikitplot as skplt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbdc8c2-6a99-4db9-bc7c-f0646550c8b8",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c500a38-5965-4b63-8bdc-2afa9541a695",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "\n",
    "There are a few steps that you must do before the actual machine learning starts. To begin with, a data scientist must analyze the quality of the data that will be used to run predictions. Biased representation of data results in a skewed model. There are several ways to analyze the data. In this tutorial, we do minimal data exploration, just enough to give an idea of what is done. We then move on to the core subject of this topic.\n",
    "\n",
    "\n",
    "## About the dataset\n",
    "In this tutorial, we use a data set that contains information about customers of an online trading platform to classify whether a given customer's probability of churn will be high, medium, or low. This provides a good example to learn how a classification model is built from start to end. The three classes that prediction will fall under are high, medium, and low. Now, let's look closer at the data set.\n",
    "\n",
    "Data is available to us in the form of a .csv file and is imported using the pandas library. We use numpy and matplotlib to get some statistics and visualize data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0cea94-cf1d-4f0a-ad16-cbde79acd233",
   "metadata": {},
   "outputs": [],
   "source": [
    "await skillsnetwork.download_dataset('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX056YEN/churn.csv')\n",
    "df_churn_pd = pd.read_csv('churn.csv')\n",
    "\n",
    "df_churn_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897e4c2a-d533-4be7-91da-e7579f189fd2",
   "metadata": {},
   "source": [
    "We first run a few lines of code to understand what data type each column is and also the number of entries in each of these columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcc2d82-a9cd-4ea7-87c4-aaf6fc5236ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset contains columns of the following data types : \\n\" + str(df_churn_pd.dtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfc783d-916e-4ba3-9684-f8ab836156ee",
   "metadata": {},
   "source": [
    "The count mismatch in the gender column (see the following image) is handled in the data preprocessing step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a1ba5b-2540-4244-933a-4c7d7bbd0e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset contains following number of records for each of the columns: \\n\" +str(df_churn_pd.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebd5d91-914f-490e-8efe-e78bbbc9eed8",
   "metadata": {},
   "source": [
    "We have plotted a basic bar chart using matplotlib to understand how data is split between the different output classes. If we are not satisfied with the representational data, now is the time to get more data to be used for training and testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c2c93d-7be2-4222-9e64-86fc2aeefa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Each category within the churnrisk column has the following count : \")\n",
    "print(df_churn_pd.groupby(['CHURNRISK']).size())\n",
    "#bar chart to show split of data\n",
    "index = ['High', 'Medium', 'Low']\n",
    "churn_plot = df_churn_pd['CHURNRISK'].value_counts(sort=True, ascending=False).plot(kind='bar',\n",
    "            figsize=(4,4), title=\"Total number for occurences of churn risk \"\n",
    "            + str(df_churn_pd['CHURNRISK'].count()), color=['#BB6B5A', '#8CCB9B', '#E5E88B'])\n",
    "churn_plot.set_xlabel(\"Churn Risk\")\n",
    "churn_plot.set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f14367-5beb-4067-a436-538cb8416f27",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "Data preprocessing is an important step in the machine learning model building process because the model can perform well only when the data it is trained on is good and well prepared. Therefore, when building models this step consumes a large amount of time. <br>\n",
    "\n",
    "There are several common data preprocessing steps that are performed in machine learning, and in this tutorial, we look at a few of them. A complete list of preprocessing options provided by scikit-learn can be found on the [scikit-learn data preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html) page. <br>\n",
    "\n",
    "We begin by identifying columns that will not add any value toward predicting the outputs. While some of these columns are easily identified, a subject matter expert is usually engaged to identify most of them. Removing such columns helps in reducing dimensionality of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250c9cc7-091b-420f-a456-e82d816a4f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns that are not required\n",
    "df_churn_pd = df_churn_pd.drop(['ID'], axis=1)\n",
    "df_churn_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c161d8-29e8-4ee9-8a53-e77a47371669",
   "metadata": {},
   "source": [
    "The preprocessing techniques that are applied must be customized for each of the columns. Sklearn provides a library called the ColumnTransformer, which allows a sequence of these techniques to be applied to selective columns using a pipeline. <br>\n",
    "\n",
    "A common problem while dealing with data sets is that values will be missing. Scikit-learn provides a method to fill these empty values with something that would be applicable in its context. We used the SimpleImputer class that is provided by Sklearn and filled the missing values with the most frequent value in the column. <br>\n",
    "\n",
    "Also, because machine learning algorithms perform better with numbers than with strings, we want to identify columns that have categories and convert them into numbers. We use the OneHotEncoder class provided by Scikit-learn. The idea of one hot encoder is to create binary variables that each represent a category. By doing this, we remove any ordinal relationship that might occur by just assigning numbers to categories. Basically, we go from a single column that contains multiple class numbers to multiple columns that contain only binary class numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c7e7ae-5d22-4277-a609-374a4d183bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the categorical columns\n",
    "categoricalColumns = ['GENDER', 'STATUS', 'HOMEOWNER']\n",
    "\n",
    "print(\"Categorical columns : \")\n",
    "print(categoricalColumns)\n",
    "\n",
    "impute_categorical = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "onehot_categorical = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[('impute', impute_categorical), ('onehot', onehot_categorical)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b91886-3b9c-45b2-b92c-677351500feb",
   "metadata": {},
   "source": [
    "The numerical columns from the data set are identified, and StandardScaler is applied to each of the columns. This way, each value is subtracted with the mean of its column and divided by its standard deviation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c658d8-61a6-43fe-adf1-01d94b72dd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the numerical columns\n",
    "numericalColumns = df_churn_pd.select_dtypes(include=[np.float,np.int]).columns\n",
    "\n",
    "print(\"Numerical columns : \" )\n",
    "print(numericalColumns)\n",
    "\n",
    "scaler_numerical = StandardScaler()\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[('scale', scaler_numerical)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051ae304-4c47-4f2f-a38d-273afb532f5c",
   "metadata": {},
   "source": [
    "As discussed previously, each of the techniques are grouped by the columns they needed to be applied on and are queued using the ColumnTransformer. Ideally, this is run in the pipeline just before the model is trained. However, to understand what the data will look like, we have transformed the data into a temporary variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9b71ec-1bb9-4ca0-a8b2-c9cca94afeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessorForCategoricalColumns = ColumnTransformer(transformers=[('cat', categorical_transformer,\n",
    "                                                                    categoricalColumns) ],\n",
    "                                            remainder=\"passthrough\" )\n",
    "preprocessorForAllColumns = ColumnTransformer(transformers=[('cat', categorical_transformer, categoricalColumns),\n",
    "                                                            ('num', numerical_transformer, numericalColumns) ],\n",
    "                                                remainder=\"passthrough\" )\n",
    "\n",
    "#. The transformation happens in the pipeline. Temporarily done here to show what intermediate value looks like\n",
    "df_churn_pd_temp = preprocessorForCategoricalColumns.fit_transform(df_churn_pd)\n",
    "\n",
    "print(\"Data after transforming :\")\n",
    "\n",
    "print(df_churn_pd_temp)\n",
    "\n",
    "df_churn_pd_temp_2 = preprocessorForAllColumns.fit_transform(df_churn_pd)\n",
    "print(\"Data after transforming :\")\n",
    "print(df_churn_pd_temp_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdc1144-5fc2-4bba-884a-76bb7fdf59fa",
   "metadata": {},
   "source": [
    "Machine learning algorithms cannot use simple text. We must convert the data from text to a number. Therefore, for each string that is a class we assign a label that is a number. For example, in the customer churn data set, the CHURNRISK output label is classified as high, medium, or low and is assigned labels 0, 1, or 2. We use the LabelEncoder class provided by Sklearn for this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c171418a-0870-4db1-acd3-fae78f0fff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data frame for splitting data into train and test datasets\n",
    "\n",
    "features = []\n",
    "features = df_churn_pd.drop(['CHURNRISK'], axis=1)\n",
    "\n",
    "label_churn = pd.DataFrame(df_churn_pd, columns = ['CHURNRISK'])\n",
    "label_encoder = LabelEncoder()\n",
    "label = df_churn_pd['CHURNRISK']\n",
    "\n",
    "label = label_encoder.fit_transform(label)\n",
    "print(\"Encoded value of Churnrisk after applying label encoder : \" + str(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6745814-0874-447a-b2ca-9a0b8eb68b1e",
   "metadata": {},
   "source": [
    "These are some of the popular preprocessing steps that are applied on the data sets. You can get more information in [Data preprocessing in detail](https://developer.ibm.com/articles/data-preprocessing-in-detail/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b530135b-50de-42cc-8d8c-46704429e42f",
   "metadata": {},
   "source": [
    "Here is the 2D and 3D view of Churnrisk data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2cf118-a72b-4d3d-bc0a-4cad4c54cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "area = 75\n",
    "x = df_churn_pd['ESTINCOME']\n",
    "y = df_churn_pd['DAYSSINCELASTTRADE']\n",
    "z = df_churn_pd['TOTALDOLLARVALUETRADED']\n",
    "\n",
    "pop_a = mpatches.Patch(color='#BB6B5A', label='High')\n",
    "pop_b = mpatches.Patch(color='#E5E88B', label='Medium')\n",
    "pop_c = mpatches.Patch(color='#8CCB9B', label='Low')\n",
    "def colormap(risk_list):\n",
    "    cols=[]\n",
    "    for l in risk_list:\n",
    "        if l==0:\n",
    "            cols.append('#BB6B5A')\n",
    "        elif l==2:\n",
    "            cols.append('#E5E88B')\n",
    "        elif l==1:\n",
    "            cols.append('#8CCB9B')\n",
    "    return cols\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "fig.suptitle('2D and 3D view of churnrisk data')\n",
    "\n",
    "# First subplot\n",
    "ax = fig.add_subplot(1, 2,1)\n",
    "\n",
    "ax.scatter(x, y, alpha=0.8, c=colormap(label), s= area)\n",
    "ax.set_ylabel('DAYS SINCE LAST TRADE')\n",
    "ax.set_xlabel('ESTIMATED INCOME')\n",
    "\n",
    "plt.legend(handles=[pop_a,pop_b,pop_c])\n",
    "\n",
    "# Second subplot\n",
    "ax = fig.add_subplot(1,2,2, projection='3d')\n",
    "\n",
    "ax.scatter(z, x, y, c=colormap(label), marker='o')\n",
    "\n",
    "ax.set_xlabel('TOTAL DOLLAR VALUE TRADED')\n",
    "ax.set_ylabel('ESTIMATED INCOME')\n",
    "ax.set_zlabel('DAYS SINCE LAST TRADE')\n",
    "\n",
    "plt.legend(handles=[pop_a,pop_b,pop_c])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc6f74-ff97-463b-97d2-438e2a09fb13",
   "metadata": {},
   "source": [
    "# Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34518982-0aef-467f-bdb9-ae2e91b0661e",
   "metadata": {},
   "source": [
    "## Splitting data for training and testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fefd76-0e18-4ad8-abfa-9b94a792f853",
   "metadata": {},
   "source": [
    "After the data has been preprocessed, the next step is to split the data into parts to be used to create and train the model and for testing and evaluating the model that is produced. There are several theories behind what percentage of data should be split between training and testing. In this tutorial, we are using 98% of the data for training and 2% of the data for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310960db-941c-48e1-88ce-df620df2afcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, label, random_state=0)\n",
    "print(\"Dimensions of datasets that will be used for training : Input features\" + str(X_train.shape) + \" Output label\" + str(y_train.shape))\n",
    "print(\"Dimensions of datasets that will be used for testing : Input features\" + str(X_test.shape) + \" Output label\" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dd15c8-89a1-4bb0-8e65-99381cdddda8",
   "metadata": {},
   "source": [
    "## Preparing a classification model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f76a9c-2222-4f12-9b7f-89a0eccc5785",
   "metadata": {},
   "source": [
    "There are several classification models that are popular and have been proven to perform with high accuracy. In this tutorial, we applied the random forest classifier by initializing the library provided by Sklearn. As part this learning path, we did a detailed description and comparison of the various classification models in [Learn classification algorithms using Python and scikit-learn](https://developer.ibm.com/tutorials/learn-classification-algorithms-using-python-and-scikit-learn/). For now, we'll skip the details of how the random forest works and continue with creating our first machine learning model.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX056YEN/random%20forest.png\" width=\"400px\" alt=\"rf\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd74c357-da84-457d-9ce3-12e670e2a93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Random Forest Classifier\"\n",
    "\n",
    "randomForestClassifier = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ea147e-199d-40f1-a58a-7e4b4c04b03c",
   "metadata": {},
   "source": [
    "## Assembling the steps using pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8da5d4-9469-40ca-b7fe-502167ae6f67",
   "metadata": {},
   "source": [
    "In this learning path, we use pipelines. Pipelines are a convenient way of designing your data processing in a machine learning flow. The idea behind using pipelines is explained in detail in [Learn classification algorithms using Python and scikit-learn](https://developer.ibm.com/tutorials/learn-classification-algorithms-using-python-and-scikit-learn/). The following code example shows how pipelines are set up using sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715ce5ce-31fc-49c5-a399-a151fb73aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_model = Pipeline(steps=[('preprocessorAll', preprocessorForAllColumns), ('classifier', randomForestClassifier)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c04c3c0-237e-45a9-b065-b9f63170f1cf",
   "metadata": {},
   "source": [
    "## Training the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d8902d-96a2-4e42-b2a9-ab9cef7656a4",
   "metadata": {},
   "source": [
    "The final step in creating the model is called modeling, where you basically train your machine learning algorithm. The 98% of data that was split in the splitting data step is used to train the model that was initialized in the previous step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9287d4d-5a75-4034-ab97-4250cea9c3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build models\n",
    "\n",
    "rfc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7ca2ba-a524-4bb5-a451-790c655b1845",
   "metadata": {},
   "source": [
    "## Running predictions on the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3113a49-c458-4517-8c37-10fc688a4520",
   "metadata": {},
   "source": [
    "After the model is trained, it is ready for some analysis. In this step, the 2% of data that was reserved for testing the model is used to run predictions. The data is blindfolded without any outputs and is passed on as shown in the following image. The predicted output is collected for evaluation against the actual results, and that is what we are doing in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710585c-4d90-45f5-86b5-d728014cc3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfc = rfc_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e54cdd-1199-4093-9750-c23901020e46",
   "metadata": {},
   "source": [
    "# Evaluating and visualizing model performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff25a397-565b-448e-91c7-9a534c83f03b",
   "metadata": {},
   "source": [
    "The prediction results acquired in the previous step are compared using what the actual results should have been. Several evaluation metrics are generated to calculate the performance of the model. <br>\n",
    "\n",
    "In a supervised classification problem such as churn risk classification, we have a true output and a model-generated predicted output for each data point. For this reason, the results for each data point can be assigned to one of four categories:\n",
    "\n",
    "1. True Positive (TP) - label is positive and prediction is also positive\n",
    "2. True Negative (TN) - label is negative and prediction is also negative\n",
    "3. False Positive (FP) - label is negative but prediction is positive\n",
    "4. False Negative (FN) - label is positive but prediction is negative\n",
    "\n",
    "These four numbers are the building blocks for most classifier evaluation metrics. A fundamental point when considering classifier evaluation is that pure accuracy (i.e. was the prediction correct or incorrect) is not generally a good metric. The reason for this is because a dataset may be highly unbalanced. For example, if a model is designed to predict fraud from a dataset where 95% of the data points are not fraud and 5% of the data points are fraud, then a naive classifier that predicts not fraud, regardless of input, will be 95% accurate. For this reason, metrics like precision and recall are typically used because they take into account the type of error. In most applications there is some desired balance between precision and recall, which can be captured by combining the two into a single metric, called the F-measure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5748646e-427a-438c-ac4a-81ed60b62c1b",
   "metadata": {},
   "source": [
    "## Helper Methods for Graph Generation\n",
    "\n",
    "Following we define some help functions to visualize the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a01816-a2f8-47ea-80ef-bb1668f8b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colormap(risk_list):\n",
    "    cols=[]\n",
    "    for l in risk_list:\n",
    "        if l==0:\n",
    "            cols.append('#BB6B5A')\n",
    "        elif l==2:\n",
    "            cols.append('#E5E88B')\n",
    "        elif l==1:\n",
    "            cols.append('#8CCB9B')\n",
    "    return cols\n",
    "\n",
    "def two_d_compare(y_test,y_pred,model_name):\n",
    "    #y_pred = label_encoder.fit_transform(y_pred)\n",
    "    #y_test = label_encoder.fit_transform(y_test)\n",
    "    area = (12 * np.random.rand(40))**2 \n",
    "    plt.subplots(ncols=2, figsize=(10,4))\n",
    "    plt.suptitle('Actual vs Predicted data : ' +model_name + '. Accuracy : %.2f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(X_test['ESTINCOME'], X_test['DAYSSINCELASTTRADE'], alpha=0.8, c=colormap(y_test))\n",
    "    plt.title('Actual')\n",
    "    plt.legend(handles=[pop_a,pop_b,pop_c])\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.scatter(X_test['ESTINCOME'], X_test['DAYSSINCELASTTRADE'],alpha=0.8, c=colormap(y_pred))\n",
    "    plt.title('Predicted')\n",
    "    plt.legend(handles=[pop_a,pop_b,pop_c])\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "x = X_test['TOTALDOLLARVALUETRADED']\n",
    "y = X_test['ESTINCOME']\n",
    "z = X_test['DAYSSINCELASTTRADE']\n",
    "\n",
    "pop_a = mpatches.Patch(color='#BB6B5A', label='High')\n",
    "pop_b = mpatches.Patch(color='#E5E88B', label='Medium')\n",
    "pop_c = mpatches.Patch(color='#8CCB9B', label='Low')\n",
    "\n",
    "def three_d_compare(y_test,y_pred,model_name):\n",
    "    fig = plt.figure(figsize=(12,10))\n",
    "    fig.suptitle('Actual vs Predicted (3D) data : ' +model_name + '. Accuracy : %.2f' % accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    ax = fig.add_subplot(121, projection='3d')\n",
    "    ax.scatter(x, y, z, c=colormap(y_test), marker='o')\n",
    "    ax.set_xlabel('TOTAL DOLLAR VALUE TRADED')\n",
    "    ax.set_ylabel('ESTIMATED INCOME')\n",
    "    ax.set_zlabel('DAYS SINCE LAST TRADE')\n",
    "    plt.legend(handles=[pop_a,pop_b,pop_c])\n",
    "    plt.title('Actual')\n",
    "\n",
    "    ax = fig.add_subplot(122, projection='3d')\n",
    "    ax.scatter(x, y, z, c=colormap(y_pred), marker='o')\n",
    "    ax.set_xlabel('TOTAL DOLLAR VALUE TRADED')\n",
    "    ax.set_ylabel('ESTIMATED INCOME')\n",
    "    ax.set_zlabel('DAYS SINCE LAST TRADE')\n",
    "    plt.legend(handles=[pop_a,pop_b,pop_c])\n",
    "    plt.title('Predicted')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def model_metrics(y_test,y_pred):\n",
    "    print(\"Decoded values of Churnrisk after applying inverse of label encoder : \" + str(np.unique(y_pred)))\n",
    "\n",
    "    skplt.metrics.plot_confusion_matrix(y_test,y_pred,text_fontsize=\"small\",cmap='Greens',figsize=(6,4))\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"The classification report for the model : \\n\\n\"+ classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56df67b-4667-4b4d-8bad-b977569c14e4",
   "metadata": {},
   "source": [
    "## 2D and 3D visualization of results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea559cb-8423-40ee-aa5a-3d3ef4d42f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_d_compare(y_test, y_pred_rfc, model_name)\n",
    "              \n",
    "three_d_compare(y_test,y_pred_rfc,model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120ba9d1-1bc8-4e9e-aa37-08fd675bd7f3",
   "metadata": {},
   "source": [
    "## Confusion matrix \n",
    "\n",
    "In the graph below we have printed a confusion matrix and a self-explanotary classification report.\n",
    "\n",
    "The confusion matrix shows that, 42 mediums were wrongly predicted as high, 2 mediums were wrongly predicted as low and 52 mediums were accurately predicted as mediums.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c7d08a-a205-4c64-ad43-446819b27866",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = label_encoder.inverse_transform(y_test)\n",
    "y_pred_rfc = label_encoder.inverse_transform(y_pred_rfc)\n",
    "model_metrics(y_test, y_pred_rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb2119c-706e-49be-a025-97e04e474aa9",
   "metadata": {},
   "source": [
    "## Comparative study\n",
    "In the bar chart below, we have compared the random forest classification algorithm output classes against the actual values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8d39d7-5e45-425d-9cfb-8e26b3ff17ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueValues, occurCount = np.unique(y_test, return_counts=True)\n",
    "frequency_actual = (occurCount[0],occurCount[2],occurCount[1])\n",
    "\n",
    "uniqueValues, occurCount = np.unique(y_pred_rfc, return_counts=True)\n",
    "frequency_predicted_rfc = (occurCount[0],occurCount[2],occurCount[1])\n",
    "\n",
    "n_groups = 3\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.1\n",
    "opacity = 0.8\n",
    "\n",
    "rects1 = plt.bar(index, frequency_actual, bar_width,\n",
    "alpha=opacity,\n",
    "color='g',\n",
    "label='Actual')\n",
    "\n",
    "rects6 = plt.bar(index + bar_width, frequency_predicted_rfc, bar_width,\n",
    "alpha=opacity,\n",
    "color='purple',\n",
    "label='Random Forest - Predicted')\n",
    "\n",
    "plt.xlabel('Churn Risk')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Actual vs Predicted frequency.')\n",
    "plt.xticks(index + bar_width, ('High', 'Medium', 'Low'))\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592b6126-c1ad-4a7c-9217-4a7101e81415",
   "metadata": {},
   "source": [
    "# Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357918b1-93b1-47f6-97ba-84b8b5a67899",
   "metadata": {},
   "source": [
    "Until evaluation provides satisfactory scores, you would repeat the data preprocessing through evaluating steps by tuning what are called the hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8ea3ba-d404-43c7-b982-fb00c6245414",
   "metadata": {},
   "source": [
    "In this tutorial, you got a hands-on example of how to develop a basic machine learning classification model from start to finish.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced7c286-2b9f-41ac-ab8b-9bd787fe7580",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aba30ba-06fe-468f-9827-09bc7ab856e4",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "<p style='color:#5A5AE5'>We recommend you add exercises to your notebook, to help your learners apply what they have learned.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30778988-9f33-42f5-b601-63ebb7193912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional (if exercises require datasets):\n",
    "\n",
    "# !wget [url for dataset used in exercises]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8d0ff2-6ce8-43f0-973f-97b19f0fe6e0",
   "metadata": {},
   "source": [
    "### Exercise 1 - Try different parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e50ae0-1ba8-435c-a47f-d41f318367a9",
   "metadata": {},
   "source": [
    "Change values of parameters on the Random Forest classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fb47c3-1f34-4efa-b146-baced4cac2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code for exercise 1 here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8c9e85-61b2-4272-8048-f116f2052134",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for solution</summary>\n",
    "    \n",
    "```python\n",
    "randomForestClassifier = RandomForestClassifier(n_estimators=50, max_depth=1, random_state=1)\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148c4825-2ce1-4eaf-8528-cb8a3ee9b8f9",
   "metadata": {},
   "source": [
    "### Exercise 2 - Changing to use other models\n",
    "Apply different classification algorithms (like Logistic Regression, Support Vector Machine, K-Nearest Neighbors) to the same dataset used in class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c680cee-98c5-4a64-94fa-57fee1b23616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code for exercise 2 here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da878734-0fe8-437c-935c-7706bf5ec1d6",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "model_name = \"KNN\"\n",
    "\n",
    "knnClassifier = KNeighborsClassifier()\n",
    "\n",
    "\n",
    "model_name = \"SVM\"\n",
    "\n",
    "svmClassifier = SVC()\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c6a964-9dbc-4482-8a18-0eee6631692d",
   "metadata": {},
   "source": [
    "### Exercise 3 - Dataset visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526043c5-a2a5-4080-a5e1-8259e98b90e0",
   "metadata": {},
   "source": [
    "Can you use some other technologies to visualize the dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36bd610-c85b-47e6-bce4-a6a821f8f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code for exercise 3 here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec64128-d397-4665-a761-a3082e57a63f",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a Hint</summary>\n",
    "\n",
    "Use [t-sne](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) technology.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0989f7e7-f444-4649-aa4d-2f2727be5667",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee434e46-6ef9-44f6-9b4c-a2b9b0cee573",
   "metadata": {},
   "source": [
    "# Congratulations! You have completed the lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05a1491-e275-4aae-9c41-ac58b37f7044",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fe9187-53b9-4fd7-b219-75399fedf001",
   "metadata": {},
   "source": [
    "[Kang Wang](https://author.skills.network/instructors/kang_wang)\n",
    "\n",
    "Kang Wang is a Data Scientist Intern in IBM. He is also a PhD Candidate in the University of Waterloo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1210d082-3b31-41da-8140-c77938422ba9",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe99b34-b766-4d83-8e58-cb7964f6c80d",
   "metadata": {},
   "source": [
    "[Joseph Santarcangelo](https://author.skills.network/instructors/joseph_santarcangelo)\n",
    "\n",
    "Joseph has a Ph.D. in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n",
    "\n",
    "[Lucy Xu](https://author.skills.network/instructors/lucy_xu)\n",
    "\n",
    "Lucy is a Data Scientist Intern at IBM. She is also currently in her fourth year at the University of Waterloo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad279866-fcfc-488d-b316-a63605debc68",
   "metadata": {},
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448483fb-90f2-423f-bc5d-77ecc30b461d",
   "metadata": {},
   "source": [
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2024-01-24|0.1|Kang Wang|Create the Project|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6e3212-2651-422e-a4e2-5872a2c1e746",
   "metadata": {},
   "source": [
    "Copyright © 2023 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "prev_pub_hash": "d10f9398147bece16576cbcb482d0643b6e95d12ed3a16020388b8f765fc0617"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
