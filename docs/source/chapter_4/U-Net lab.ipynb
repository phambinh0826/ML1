{"cells":[{"cell_type":"markdown","id":"2ea1bdaa-307b-4217-9a58-fe588a3b96af","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"d98f04cc-b254-47a0-85c0-b24623a495aa","metadata":{},"outputs":[],"source":["# **Predict credit defaults with random forest using Python**\n"]},{"cell_type":"markdown","id":"1817f70e-ce2e-4e06-9ab5-2ebb0eee3783","metadata":{},"outputs":[],"source":["Estimated time needed: **30** minutes\n"]},{"cell_type":"markdown","id":"a6c53395-2e39-47f9-be7b-e7b835e327b0","metadata":{},"outputs":[],"source":["This project is based on the [IBM developer tutorial: Using random forest to predict credit defaults using Python](https://developer.ibm.com/tutorials/awb-random-forest-predict-credit-defaults/) by Karina Kervin.\n"]},{"cell_type":"markdown","id":"4a31a24c-112a-4829-bcec-0af1a03bbbce","metadata":{},"outputs":[],"source":["## Introduction\n","\n","You work for a company that provides car loans with interest rates. Recently, loan underwriters have been issuing a significant number of loans that end up defaulting. As a result, there’s a need to improve the underwriting process. To address this issue, you decide to use a random forest algorithm to enhance the accuracy and reliability of loan approvals.\n","\n","<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/fKgtqohuI-r7pZRuuBihew/dall--e-2024-05-28-10-28-24---a-well-dressed-man-standing-next-to-a-luxury-car-in-front-of-a-rundown--dilapidated-house--the-car-is-polished-and-gleaming--contrasting-sharply-with-.jpg\" alt=\"Image\" width=\"500\"/>\n","\n"," \n"]},{"cell_type":"markdown","id":"9fe8e2aa-fe06-482f-8a97-bdedd71f3648","metadata":{},"outputs":[],"source":["## What does this guided project do?\n","\n","This project begins with an exploration of data loading and preprocessing techniques. You learn the essentials of handling missing values, encoding categorical variables, and normalizing numerical features by using pandas and scikit-learn. This ensures that your data is pristine and ready for the magic of machine learning.\n","\n","Next, you unleash the power of the random forest algorithm. You work through the process of splitting your data into training and testing sets, fitting the model, and making predictions. You discover how random forest's ensemble approach, which merges multiple decision trees, enhances prediction accuracy and robustness, especially for complex data sets.\n","\n","But, you won't stop there. Understanding your model's performance is crucial. So, you'll delve into evaluating the model using metrics. \n","\n","To take your model to the next level, you're introduced to advanced techniques for fine-tuning by using hyperparameter adjustment to enhance model performance.\n","\n","By the end of this tutorial, you are armed with a comprehensive understanding of implementing and evaluating a random forest model for predicting credit defaults, showcasing the practical benefits of this method in real-world financial scenarios. This project is designed not just to teach you, but to inspire you with the real-world applications of machine learning in the banking industry.\n","\n","This tutorial is perfect for data science enthusiasts, financial analysts, and machine learning practitioners with a foundational knowledge of Python and machine learning principles.\n"]},{"cell_type":"markdown","id":"15dfd0be-0d4f-4ae7-b5c2-07023ab3725a","metadata":{},"outputs":[],"source":["## Background\n"]},{"cell_type":"markdown","id":"23e6051d-ff04-483d-9e42-92623d52815b","metadata":{},"outputs":[],"source":["### What is random forest\n","\n","\n","[Random forest](https://www.ibm.com/topics/random-forest?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-Random+Forest+Lab_v1_1717603754) is a commonly-used **machine learning algorithm**, trademarked by Leo Breiman and Adele Cutler, that combines the output of multiple **decision trees** to reach a single result. Its ease of use and flexibility have fueled its adoption, as it handles both classification and regression problems.\n"]},{"cell_type":"markdown","id":"7ab35509-d80f-4162-870c-f6240b28e159","metadata":{},"outputs":[],"source":["### How it works\n","\n","Random forest algorithms have three main hyperparameters, which must be set before training:\n","+ Node size\n","+ The number of trees\n","+ The number of features sampled\n","\n","From there, you can use the random forest classifier to solve for regression or classification problems.\n","\n","The following image shows a typical random forest.\n","\n","<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/wmw8tYbZuTATfF1z5e0phA/random-forest.png\" alt=\"Image\" width=\"800\"/>\n","\n"]},{"cell_type":"markdown","id":"9535443c-06b5-45eb-bf8b-ae288d403177","metadata":{},"outputs":[],"source":["### What is a decision tree\n","\n","A [decision tree](https://www.ibm.com/topics/decision-trees?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-Random+Forest+Lab_v1_1717603754) is a non-parametric **supervised learning algorithm** that is utilized for both classification and regression tasks. It has a hierarchical, tree structure, which consists of a root node, branches, internal nodes, and leaf nodes.\n","\n","The following image shows an example of real situation. Imagine that you were trying to assess whether you should go surfing. You might use the following decision rules to make a choice.\n","\n","<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/xRWAXqej4sL8eQUsJD6iqw/ICLH-Diagram-Batch-03-24A-AI-ML-DecisionTree.png\" alt=\"Image\" width=\"800\"/>\n"]},{"cell_type":"markdown","id":"c007a588-ac7a-4d90-a248-58eb42209623","metadata":{},"outputs":[],"source":["## Table of contents\n","\n","1. [Objectives](#Objectives)\n","2. [Setup](#Setup)\n","   1. [Installing required libraries](#Installing-Required-Libraries)\n","   2. [Importing required libraries](#Importing-Required-Libraries)\n","3. [Import data](#Import-Data)\n","4. [Analyze missing data](#Analyze-missing-data)\n","5. [Downsample the data set](#Downsample-the-data-set)\n","6. [Hot encode the independent variables](#Hot-encode-the-independent-variables)\n","7. [Split the data set](#Split-the-data-set)\n","8. [Classify accounts and evaluate the model](#Classify-accounts-and-evaluate-the-model)\n","9. [Optimize the model with hyperparameter tuning](#Optimize-the-model-with-hyperparameter-tuning)\n","10. [Summary](#Summary)\n"]},{"cell_type":"markdown","id":"3c8fe9e4-1430-4152-b588-e92a0871fce0","metadata":{},"outputs":[],"source":["## Objectives\n","\n","After you complete the project, you:\n","\n","- Understand the fundamentals of the **random forest algorithm** and its application in **financial data science**.\n","- Gain proficiency in using **Python** and the **scikit-learn** library to build predictive models.\n","- Develop skills in preparing financial data for machine learning tasks, including data cleaning and feature engineering.\n","- Learn to **evaluate** model performance using metrics.\n"]},{"cell_type":"markdown","id":"5cfcfbff-d9f0-4c70-88c6-3734daa54a08","metadata":{},"outputs":[],"source":["----\n"]},{"cell_type":"markdown","id":"7989c387-9121-4fcb-8754-b91c564cf0d0","metadata":{},"outputs":[],"source":["## Setup\n"]},{"cell_type":"markdown","id":"b68d56ad-b854-4a79-8e63-1dca7fcdb6eb","metadata":{},"outputs":[],"source":["For this lab, you use the following libraries:\n","\n","*   [`pandas`](https://pandas.pydata.org/) for managing the data.\n","*   [`numpy`](https://numpy.org/) for mathematical operations.\n","*   [`sklearn`](https://scikit-learn.org/) for machine learning and machine learning pipeline-related functions.\n","*   [`seaborn`](https://seaborn.pydata.org/) for visualizing the data.\n","*   [`matplotlib`](https://matplotlib.org/) for additional plotting tools.\n","*   [`xlrd`](https://xlrd.readthedocs.io/en/latest/) for reading data and formatting information from Excel files in the historical `.xls` format.\n"]},{"cell_type":"markdown","id":"9e7aba0b-1b16-4367-94f2-2f16df7357eb","metadata":{},"outputs":[],"source":["### Installing required libraries\n"]},{"cell_type":"markdown","id":"d1a9ee7e-4545-4e4f-9b9d-df5d161fd3c3","metadata":{},"outputs":[],"source":["The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You must run the following cell__ to install them. Please wait until it is complete.\n"]},{"cell_type":"code","id":"e6a2a1ce-7469-4973-a805-384bb4f93cd3","metadata":{},"outputs":[],"source":["# Install necessary packages\n%pip install -q \\\n    numpy==1.26.4 \\\n    pandas==2.2.2 \\\n    matplotlib==3.9.0 \\\n    seaborn==0.13.2 \\\n    scikit-learn==1.4.2 \\\n    xlrd==2.0.1"]},{"cell_type":"markdown","id":"1e1edeb9-dd2d-4ca1-99fe-cd82cf673635","metadata":{},"outputs":[],"source":["### Importing required libraries\n"]},{"cell_type":"markdown","id":"b85c9ee9-bb9f-4ffa-9972-4735a9e6fd86","metadata":{},"outputs":[],"source":["The following code cell accomplishes two tasks: it **imports the required libraries** and **loads a YouTube video**. This video provides a fundamental understanding of how random forests operate.\n","\n","**It’s strongly recommended that you take the time to watch it!**\n"]},{"cell_type":"code","id":"8a2475fb-2190-47d9-b41b-bb7ad7a03a64","metadata":{},"outputs":[],"source":["import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport warnings\n\n# Suppress warnings:\ndef warn(*args, **kwargs):\n    pass\n\nwarnings.warn = warn\nwarnings.filterwarnings('ignore')\n\nsns.set_context('notebook')\nsns.set_style('white')\n\n# The following code loads the YouTube video:\nfrom IPython.display import YouTubeVideo\nYouTubeVideo('gkXX4h3qYm4', width=800, height=452)"]},{"cell_type":"markdown","id":"7be7110d-5e57-4f30-9dc7-2f1aac47dc60","metadata":{},"outputs":[],"source":["### Import data\n"]},{"cell_type":"markdown","id":"515fe33d-0561-4186-9610-ee740f82f756","metadata":{},"outputs":[],"source":["The task for this tutorial is to predict the probability of a customer defaulting on a loan. The target variable is \"default payment.\" Details of the data measurements can be found in [UCI's data repository](https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-random-forest-predict-credit-defaults).\n","\n","This step might take a couple of minutes to complete. Please be patient.\n"]},{"cell_type":"code","id":"1b3736f9-12a7-4805-a880-4be99401b0bd","metadata":{},"outputs":[],"source":["# Import the data set\ndf = pd.read_excel('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/UEym8G6lwphKjuhkLgkXAg/default%20of%20credit%20card%20clients.xls', header=1)"]},{"cell_type":"markdown","id":"798c9367-63b9-4768-abab-48e3f3aa344b","metadata":{},"outputs":[],"source":["Exploring the data set is a critical step that should not be overlooked. This can help you to understand what data is available and the quality of that data. This guides both data cleaning and modeling decisions. First, let's look at the first five rows of the DataFrame.\n"]},{"cell_type":"code","id":"f183c2be-f823-4eb3-932e-d5c107686790","metadata":{},"outputs":[],"source":["# Explore the first five rows of the data set\ndf.head(5)"]},{"cell_type":"markdown","id":"8a2f0acf-4e05-4d3f-ac8e-3d268c853aef","metadata":{},"outputs":[],"source":["Each row represents an individual. Some columns that could be useful are:\n","+ Credit limit (**LIMIT_BAL**)\n","+ Prior payment status (**PAY_0,...,PAY_6**)\n","+ Bill and payment amounts (**BILL_AMT, PAY_AMT**)\n","+ Target variable that indicates default next month (**default payment next month**)\n","\n","The following code renames the default column. The ID column is also dropped because it doesn't contain any information relevant to the analysis.\n"]},{"cell_type":"code","id":"20863867-133e-444a-aa7d-69df3bfac211","metadata":{},"outputs":[],"source":["# Rename the columns\ndf.rename({'default payment next month': 'DEFAULT'}, axis='columns', inplace=True)\n\n#Remove the ID column as it is not informative\ndf.drop('ID', axis=1, inplace=True)\ndf.head()"]},{"cell_type":"markdown","id":"11e50e44-96bc-4626-a73e-93633b385b7a","metadata":{},"outputs":[],"source":["### Analyze missing data\n","\n","One key step is to check for null values or other invalid input that will cause the model to throw an error. \n"]},{"cell_type":"code","id":"c4342f8e-e6b0-4af1-ac90-453be7fdaa16","metadata":{},"outputs":[],"source":["# Check dimensions for invalid values\nprint(f\"SEX values include: {df['SEX'].unique()}\")\nprint(f\"MARRIAGE values include: {df['MARRIAGE'].unique()}\")\nprint(f\"EDUCATION values include: {df['EDUCATION'].unique()}\")\n\n# Count missing or null values\nprint(f\"Number of missing values in SEX: {len(df[pd.isnull(df.SEX)])}\")\nprint(f\"Number of missing values in MARRIAGE: {len(df[pd.isnull(df.MARRIAGE)])}\")\nprint(f\"Number of missing values in EDUCATION: {len(df[pd.isnull(df.EDUCATION)])}\")\nprint(f\"Number of missing values in AGE: {len(df[pd.isnull(df.AGE)])}\")\n\n# Count of invalid data in EDUCATION and MARRIAGE\ninvalid_count = len(df.loc[(df['EDUCATION'] == 0) | (df['MARRIAGE'] == 0)])\nprint(f\"Number of invalid data points in EDUCATION or MARRIAGE: {invalid_count}\")\n"]},{"cell_type":"markdown","id":"8553c3f4-71bd-413a-9d19-8297601dac10","metadata":{},"outputs":[],"source":["The output indicates that some of the data does not align with the data definitions, specifically the EDUCATION and MARRIAGE columns. \n","\n","* **EDUCATION** includes three types of invalid values, which are 0, 5, and 6.\n","* **MARRIAGE** includes 0 as an invalid value. \n","\n","Assume that a 0 encoding is supposed to represent missing data and that a value of 5 or 6 within EDUCATION is representative of other unspecified education levels (for example, Ph.D. or a master's degree), which is not represented within the data definition.\n","**68** rows exist in the DataFrame where either the **EDUCATION** or the **MARRIAGE** column is zero. \n","\n","Next, let's filter the rows where the EDUCATION and MARRIAGE columns have non-zero values. \n","\n","The following code creates a new DataFrame with the missing values for **EDUCATION** and **MARRIAGE** removed. We end up with 29,932 rows remaining.\n"]},{"cell_type":"code","id":"eef01be3-5658-464d-a8a9-84541c8f125d","metadata":{},"outputs":[],"source":["print(f\"shape of data: {df.shape}\")\n\n#Filter the DataFrame\ndf_no_missing_data = df.loc[(df['EDUCATION'] != 0) & (df['MARRIAGE'] != 0)]\nprint(f\"shape of no_missing_data: {df_no_missing_data.shape}\")"]},{"cell_type":"markdown","id":"ae87655f-c009-4087-94cb-b7f339e9f908","metadata":{},"outputs":[],"source":["The next step is to check if the target variable, which indicates whether someone defaulted, is balanced.\n","The chart shows counts of people who have defaulted (1) and haven't defaulted (0). \n","Unsurprisingly, most people have not defaulted on their loans.\n","\n","To address this class imbalance, you must down sample the data.\n"]},{"cell_type":"code","id":"b5efe9f2-a0ef-4bd3-bbd6-4ae431d5e2b7","metadata":{},"outputs":[],"source":["# Explore distribution of data set\n# count plot on ouput variable\nax = sns.countplot(x = df_no_missing_data['DEFAULT'], palette = 'rocket')\n\n#add data labels\n# ax.bar_label(ax.containers[0])\nfor container in ax.containers:\n    ax.bar_label(container)\n\n# add plot title\nplt.title(\"Observations by Classification Type\")\n\n# show plot\nplt.show()"]},{"cell_type":"markdown","id":"10a07886-5c2a-4f4e-b038-dd570dcc23e1","metadata":{},"outputs":[],"source":["### Downsample the data set\n","\n","The first step in downsampling is to split the data based on those who defaulted on their loan and those who did not default on their loan. \n","\n","You will randomly select 1,000 samples from each category. \n","\n","The two data sets are then merged back together to create an analysis data set.\n"]},{"cell_type":"code","id":"d4f36b17-a34c-4bf7-85f8-197101358c35","metadata":{},"outputs":[],"source":["# split data\ndf_no_default = df_no_missing_data.loc[(df_no_missing_data['DEFAULT']==0)]\ndf_default = df_no_missing_data.loc[(df_no_missing_data['DEFAULT']==1)]\n\n# downsample the data set\ndf_no_default_downsampled = resample(df_no_default, replace=False, n_samples=1000, random_state=0)\ndf_default_downsampled = resample(df_default, replace=False, n_samples=1000, random_state=0)\n\n#check ouput\nprint(f\"Length of df_no_default_downsampled: {len(df_no_default_downsampled)}\")\nprint(f\"Length of df_default_downsampled: {len(df_default_downsampled)}\")\n\n# merge the data sets\ndf_downsample = pd.concat([df_no_default_downsampled, df_default_downsampled ])\nprint(f\"Shape of df_downsample: {df_downsample.shape}\")"]},{"cell_type":"markdown","id":"837551f0-fee3-4a8a-b6c9-35e5b2005e22","metadata":{},"outputs":[],"source":["### Hot encode the independent variables\n","\n","Next, you convert each category into a binary variable with a value of 0 or 1. Pandas has a very convenient function to do just this, called `get_dummies`.\n","\n","**Why hot encode?**\n","\n","* Improved model performance\n","* Avoid implicit ordering\n","* Compatibility and consistency\n","\n","One thing to keep in mind when creating models is to avoid bias. One very important way to do this is to not use variables associated with protected attributes as independent variables. In this case, SEX, AGE, and MARRIAGE clearly fall into that category. EDUCATION is somewhat more ambiguous. Because it is not critical for the purposes of this tutorial, this is dropped as well.\n"]},{"cell_type":"code","id":"766607c8-1446-4f4c-b5b4-7d593700af29","metadata":{},"outputs":[],"source":["# isolate independent variables\nX = df_downsample.drop(['DEFAULT','SEX', 'EDUCATION', 'MARRIAGE','AGE'], axis=1).copy()\nprint(f\"Shape of X: {X.shape}\")\n\n# NOTE: 'PAY_1' is not shown in original data\nX_encoded = pd.get_dummies(data=X, columns=['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6'])\nprint(f\"Shape of X_encoded: {X_encoded.shape}\")\nX_encoded.head()"]},{"cell_type":"markdown","id":"8dab4912-706b-4ae5-9b6d-434c55317216","metadata":{},"outputs":[],"source":["### Split the data set\n","\n","Splitting the data into test and training sets is critical for understanding how your model performs on new data. \n","The random forest model uses the training data set to learn what factors should become decision nodes. \n","The test set helps you evaluate how often those decisions lead to the correct decision.\n"]},{"cell_type":"code","id":"2eaf1245-3394-40ef-a4b9-d4096b3ae3f9","metadata":{},"outputs":[],"source":["# Split the data\ny = df_downsample['DEFAULT'].copy()\nX_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=0)\n\nprint(f'X_train.shape: {X_train.shape}')\nprint(f'X_test.shape: {X_test.shape}')\nprint(f'y_train.shape: {y_train.shape}')\nprint(f'y_test.shape: {y_test.shape}')"]},{"cell_type":"markdown","id":"8a109746-730b-47a3-b10a-c646d9e55a5c","metadata":{},"outputs":[],"source":["### Classify accounts and evaluate the model\n","\n","Now, it's time to build an initial random forest model by fitting it by using the training data and evaluating the resulting model using the test data. To make that evaluation easier, you plot the results using a [confusion matrix](https://www.ibm.com/topics/confusion-matrix?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-Random+Forest+Lab_v1_1717603754).\n"]},{"cell_type":"code","id":"a0f9635b-ab13-4d7a-98a4-9e2a3a72bf41","metadata":{},"outputs":[],"source":["# apply RandomForestClassifier\nclf_rf = RandomForestClassifier(max_depth=2, random_state=0)\nclf_rf.fit(X_train, y_train)\n\n#calculate overall accuracy\ny_pred = clf_rf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy:.2%}')\n\nclass_names = ['Did Not Default', 'Defaulted']\n\n# Generate the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Calculate the percentage of correctly predicted instances for each class\nfor i, class_name in enumerate(class_names):\n    correct_predictions = cm[i, i]\n    total_predictions = cm[i, :].sum()\n    class_accuracy = correct_predictions / total_predictions * 100\n    print(f'Percentage of correctly predicted {class_name}: {class_accuracy:.2f}%')\n\n# Display the confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\ndisp.plot(cmap=plt.cm.Blues)\nplt.show()"]},{"cell_type":"markdown","id":"b61cc669-961c-4136-9ea3-70ad9e51e6b7","metadata":{},"outputs":[],"source":["As you can see, the model performance could be improved. \n"]},{"cell_type":"markdown","id":"96677ff6-9603-4496-a1a7-eec7925c3841","metadata":{},"outputs":[],"source":["### Optimize the model with hyperparameter tuning\n","\n","Cross validation and GridSearchCV are useful tools for finding better hyperparameters for models. When it comes to random forest models, you'll focus on `max_depth`, `min_samples_split`, `min_samples_leaf`. Here's a quick overview of what those hyperparameters mean:\n","\n","- `max_depth`: The maximum number levels the decision trees that make up the random forest are allowed to have\n","- `min_samples_split`: The minimum number of samples that must be in a node for a decision split to be allowed\n","- `min_samples_leaf`: The minimum number of samples that must exist in a leaf node\n","\n","\n","Another commonly used hyperparameter is `max_features`. This is the number of features that the model will try out when attempting to create a decision node. The `n_estimators` hyperparameter controls the number of decision trees that are created as part of the random forest model. For more details and other hyperparameters that can be tuned, see the [sklearn random forest documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n"]},{"cell_type":"code","id":"47658061-bd0b-4b9d-a746-247fb788f185","metadata":{},"outputs":[],"source":["param_grid = {\n    'max_depth':[3,4,5],\n    'min_samples_split':[3,4,5],\n    'min_samples_leaf':[3,4,5],\n}\n\nrf_random = RandomizedSearchCV(\n    estimator=clf_rf, \n    param_distributions=param_grid, \n    n_iter=27, \n    cv=3, \n    random_state=0, \n    verbose=1,\n    n_jobs = -1,\n)\n\n# Fit the random search model\nrf_random.fit(X_train, y_train)\n\n# Output the best hyperparameters found\nbest_params = rf_random.best_params_\nprint(f'Best parameters found: {best_params}')\nprint(f'Best estimator is: {rf_random.best_estimator_}')\n\n# Refit the model using the best hyperparameters\nbest_clf_rf = rf_random.best_estimator_\n\n# In case you want to check all parameters currently in use\n# print(f'Parameters currently in use: {best_clf_rf.get_params()}')\n\n# Train the refitted model\nbest_clf_rf.fit(X_train, y_train)\n\n# Calculate overall accuracy\ny_pred = best_clf_rf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy:.2%}')\n\n# Plot the confusion matrix\nclass_names = ['Did Not Default', 'Defaulted']\ndisp = ConfusionMatrixDisplay.from_estimator(\n    best_clf_rf,\n    X_test,\n    y_test,\n    display_labels=class_names,\n    cmap=plt.cm.Blues,\n)\nplt.show()"]},{"cell_type":"markdown","id":"df3a1278-6eea-4863-95f5-f17fe81314d1","metadata":{},"outputs":[],"source":["Just by trying a few different hyperparameters, you improved the accuracy by **3%**. \n"]},{"cell_type":"markdown","id":"efb91c53-8b37-4ef2-89ca-a0c60b20c1c6","metadata":{},"outputs":[],"source":["## Summary\n","\n","In this tutorial, you learned how to apply random forest classification to predict credit card defaults. You also fine tuned your classifier model by optimizing the hyperparameters, which can improve the accuracy.\n"]},{"cell_type":"markdown","id":"ef6e22b2-a1b1-44fe-a1ed-76e7ce098bb6","metadata":{},"outputs":[],"source":["# Exercises\n"]},{"cell_type":"markdown","id":"54df39f7-30a3-4a31-844d-868768593564","metadata":{},"outputs":[],"source":["### Exercise - Try different hyperparameters\n"]},{"cell_type":"code","id":"686a4708-c2d0-4921-ab44-442db161a180","metadata":{},"outputs":[],"source":["# Try different hyperparameters\nclf_rf = RandomForestClassifier(\n    max_depth=5,         # change number here \n    min_samples_split=5, # change number here \n    min_samples_leaf=5,  # change number here\n    random_state=0,\n)\nclf_rf.fit(X_train, y_train)\n\n# calculate overall accuracy\ny_pred = clf_rf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy:.2%}')"]},{"cell_type":"markdown","id":"735b7ccb-0aa2-4479-acc6-feb460942a94","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for solution</summary>\n","\n","```python\n","# refit model with different hyperparameters\n","clf_rf = RandomForestClassifier(\n","    max_depth=3,         # change number here \n","    min_samples_split=3, # change number here \n","    min_samples_leaf=3,  # change number here\n","    random_state=0,\n",")\n","clf_rf.fit(X_train, y_train)\n","\n","#calculate overall accuracy\n","y_pred = clf_rf.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print(f'Accuracy: {accuracy:.2%}')\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"2b9d87c9-a930-4983-8741-c12ee77a68a5","metadata":{},"outputs":[],"source":["## Authors\n"]},{"cell_type":"markdown","id":"a25f168a-eae0-42e3-8875-e912d1a49645","metadata":{},"outputs":[],"source":["[Ricky Shi](https://author.skills.network/instructors/ricky_shi)\n","\n","[Karina Kervin](https://author.skills.network/instructors/karina_kervin)\n"]},{"cell_type":"markdown","id":"8d0837e3-dd17-4467-a525-f59fe03906d0","metadata":{},"outputs":[],"source":["### Other Contributors\n"]},{"cell_type":"markdown","id":"af09f76c-a9f8-4835-a546-68fd6f1ba03d","metadata":{},"outputs":[],"source":["[Wojciech \"Victor\" Fulmyk](https://www.linkedin.com/in/wfulmyk)\n","\n","[Kang Wang](https://author.skills.network/instructors/kang_wang)\n"]},{"cell_type":"markdown","id":"ddd061af-362d-461e-ac71-f96b84aa7550","metadata":{},"outputs":[],"source":["Copyright © 2020 IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"prev_pub_hash":"fedf7d0ba108d327351ba360f355e5e89b335e3f5f5ffb574384907434076129"},"nbformat":4,"nbformat_minor":4}